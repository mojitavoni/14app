# Minimal Developer Environment - DataOps

![License](https://img.shields.io/badge/license-MIT-blue.svg)
![Size](https://img.shields.io/badge/size-~450MB-green.svg)
![User](https://img.shields.io/badge/user-tmj-orange.svg)

> **Ultra-lightweight, production-ready DataOps environment for modern data engineering workflows**

## üéØ What is This?

A **minimal Alpine-based development container** (~450MB) optimized for:
- üìä **Data Engineering** - ETL/ELT pipelines, data transformation
- üóÑÔ∏è **Multi-format support** - Parquet, Delta Lake, CSV, JSON, XML, Avro, Excel
- üê≥ **Containerized workflows** - Docker-in-Docker for scalable pipelines
- üîç **Data quality** - Validation, profiling, testing
- üöÄ **Production-ready** - CI/CD, monitoring, best practices

## ‚ú® What's Included

| Component | Size | Purpose |
|-----------|------|---------|
| **Alpine Linux 3.19** | 5MB | Minimal, secure base |
| **Python 3.12** | 50MB | Data processing core |
| **Node.js 20 LTS** | 80MB | Tooling & APIs |
| **PowerShell Core** | 60MB | Cross-platform scripting |
| **Docker-in-Docker** | 100MB | Containerized pipelines |
| **DuckDB + extensions** | 30MB | Analytics database |
| **DB Clients** | 17MB | PostgreSQL, MySQL, SQLite |
| **DataOps Libraries** | 108MB | Delta Lake, dbt, Great Expectations |
| **Total** | **~450MB** | Complete environment |

---

## üì¶ Supported Data Formats

### ‚úÖ All Common Formats

| Format | Libraries | Use Case |
|--------|-----------|----------|
| **CSV/TSV** | pandas, polars, csvkit, DuckDB | Universal interchange |
| **JSON/JSONL** | pandas, jq, Node.js | APIs, logs, config |
| **Parquet** | pyarrow, fastparquet, DuckDB | Analytics (10x smaller than CSV) |
| **Delta Lake** | deltalake, DuckDB | ACID transactions, time travel |
| **XML** | lxml, xmltodict, xmlstarlet | Legacy systems, SOAP |
| **Excel** | openpyxl | Business data |
| **Avro** | avro-python3, fastavro | Schema evolution |
| **Protobuf** | protobuf | gRPC, efficient binary |
| **YAML** | pyyaml, yq | Configuration |
| **Text** | awk, sed, grep, regex | Log processing |

---

## üöÄ Quick Start

```bash
# 1. Open in VS Code
code .

# 2. Rebuild container
# Press F1 ‚Üí "Dev Containers: Rebuild Container"
# Wait 8-10 minutes for first build

# 3. Verify installation
sh scripts/test_setup.sh

# 4. Start building pipelines!
```

---

## üí° Usage Examples

### Delta Lake - ACID Transactions

```python
from deltalake import DeltaTable, write_deltalake
import pandas as pd
import duckdb

# Create Delta table
df = pd.read_csv('data/raw/sales.csv')
write_deltalake('data/processed/sales_delta', df, mode='overwrite')

# Query with DuckDB (fast!)
con = duckdb.connect()
con.execute("INSTALL delta; LOAD delta;")
result = con.execute("""
    SELECT category, SUM(amount) as total
    FROM delta_scan('data/processed/sales_delta')
    GROUP BY category
""").df()

# Time travel - query historical versions
dt = DeltaTable('data/processed/sales_delta', version=0)
old_data = dt.to_pandas()
```

### Multi-Format Pipeline

```python
from scripts.python.multi_format_pipeline import DataPipeline

pipeline = DataPipeline()

# Read any format
df_csv = pipeline.read_csv('data/raw/input.csv')
df_json = pipeline.read_json('data/raw/input.json')
df_xml = pipeline.read_xml('data/raw/input.xml')
df_excel = pipeline.read_excel('data/raw/input.xlsx')

# Transform with DuckDB SQL
result = pipeline.transform_with_duckdb("""
    SELECT category, COUNT(*) as count, SUM(amount) as total
    FROM df_csv
    WHERE date >= '2024-01-01'
    GROUP BY category
""")

# Write optimized Parquet (10x smaller!)
pipeline.write_parquet(result, 'summary.parquet')
```

### Data Quality Validation

```python
from scripts.python.data_quality import validate_data_quality

df = pd.read_csv('data/raw/input.csv')
results = validate_data_quality(df)

if results['failed']:
    raise ValueError("Data quality checks failed!")
# Continue processing...
```

### Containerized Pipeline (Scalable!)

```bash
# Run entire pipeline in Docker containers
sh scripts/shell/containerized_pipeline.sh

# Benefits:
# - Reproducible (same results every time)
# - Isolated (no dependency conflicts)
# - Scalable (deploy to Kubernetes, ECS, etc.)
# - Portable (works locally and in production)
```

---

## üèóÔ∏è Project Structure

```
14app/
‚îú‚îÄ‚îÄ .devcontainer/              # Container configuration
‚îÇ   ‚îú‚îÄ‚îÄ devcontainer.json       # Alpine + Docker + DataOps
‚îÇ   ‚îú‚îÄ‚îÄ setup.sh                # Installation script
‚îÇ   ‚îî‚îÄ‚îÄ README.md               # Technical docs
‚îú‚îÄ‚îÄ .github/workflows/
‚îÇ   ‚îî‚îÄ‚îÄ ci.yml                  # CI/CD pipeline
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îú‚îÄ‚îÄ raw/                    # Source data (gitignored)
‚îÇ   ‚îú‚îÄ‚îÄ processed/              # Delta tables, transformed
‚îÇ   ‚îî‚îÄ‚îÄ output/                 # Results
‚îú‚îÄ‚îÄ scripts/
‚îÇ   ‚îú‚îÄ‚îÄ python/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ delta_operations.py       # Delta Lake examples
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ multi_format_pipeline.py  # Format conversion
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ data_quality.py           # Validation
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ process_data.py           # DuckDB processing
‚îÇ   ‚îú‚îÄ‚îÄ nodejs/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ convert_csv.js            # Node.js utilities
‚îÇ   ‚îú‚îÄ‚îÄ shell/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ containerized_pipeline.sh # Docker pipeline
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ process_data.sh           # Bash processing
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ text_processing.sh        # awk/sed/grep
‚îÇ   ‚îî‚îÄ‚îÄ test_setup.sh                 # Verify installation
‚îú‚îÄ‚îÄ .env.example                # Environment template
‚îî‚îÄ‚îÄ README.md                   # This file
```

---

## üê≥ Scaling to Production

### Local Development
```bash
sh scripts/shell/containerized_pipeline.sh
```

### Kubernetes
```yaml
apiVersion: batch/v1
kind: Job
metadata:
  name: data-pipeline
spec:
  template:
    spec:
      containers:
      - name: dataops
        image: your-registry/dataops:latest
        command: ["python", "scripts/python/process_data.py"]
```

### AWS ECS / Azure Container Instances
```bash
# Build and push image
docker build -t dataops:latest .
docker push your-ecr/dataops:latest

# Deploy as ECS task
aws ecs run-task --task-definition dataops-pipeline
```

### Serverless (AWS Lambda)
```python
def lambda_handler(event, context):
    from scripts.python.multi_format_pipeline import DataPipeline
    pipeline = DataPipeline()
    # Process data triggered by S3, SQS, etc.
```

---

## üìä Performance Comparison

### File Size (1M rows)
| Format | Size | Read Time |
|--------|------|-----------|
| CSV | 100 MB | 5 sec |
| JSON | 150 MB | 8 sec |
| **Parquet** | **10 MB** | **0.5 sec** |
| **Delta Lake** | **10 MB** | **0.5 sec** + ACID |

### Processing Speed (10M rows)
| Tool | Time | Memory |
|------|------|--------|
| Pandas | 30 sec | 8 GB |
| **Polars** | **6 sec** | **2 GB** |
| **DuckDB** | **3 sec** | **500 MB** |

---

## üîß DataOps Tools

### Data Transformation
- **dbt-core** + **dbt-duckdb** - SQL-based transformations
- **DuckDB** - In-process analytics (no server needed)
- **Polars** - Fast DataFrame library (5-10x faster than Pandas)

### Data Quality
- **Great Expectations** - Comprehensive validation framework
- **JSON Schema** - JSON validation
- Custom validators (see `scripts/python/data_quality.py`)

### Data Orchestration (Compatible)
- Airflow, Prefect, Dagster
- Kubernetes CronJobs
- GitHub Actions
- AWS Step Functions

---

## üîí Security & Best Practices

### ‚úÖ Security
- **Non-root user** - Runs as `tmj`
- **Minimal packages** - No unnecessary tools
- **Secret management** - `.env` files (never committed)
- **CI/CD scanning** - Trivy security scanner
- **Container isolation** - Docker-in-Docker

### ‚úÖ DataOps Best Practices
- **Version control** - Git for code, Delta Lake for data
- **Data validation** - Quality checks before processing
- **Reproducibility** - Containerized pipelines
- **Testing** - Unit tests, integration tests, data tests
- **Monitoring** - Logging, metrics, alerts

---

## üéì Common Tasks

### Convert CSV to Parquet (10x smaller)
```bash
duckdb -c "COPY (SELECT * FROM 'data.csv') TO 'data.parquet' (FORMAT PARQUET, COMPRESSION SNAPPY)"
```

### Query Multiple Files
```bash
duckdb -c "SELECT * FROM 'sales/*.parquet' WHERE amount > 1000"
```

### Join Files
```bash
duckdb -c "
  SELECT * FROM 'sales.csv' s
  JOIN 'customers.csv' c ON s.customer_id = c.id
"
```

### Process XML
```bash
# Extract specific elements
xmlstarlet sel -t -v "//item/name" data.xml

# Convert XML to JSON
python -c "
import xmltodict, json
with open('data.xml') as f:
    data = xmltodict.parse(f.read())
print(json.dumps(data, indent=2))
"
```

### Text Processing
```bash
# Extract emails
grep -oE '[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}' logs.txt

# Sum column with awk
awk -F',' '{sum+=$3} END {print sum}' data.csv

# Remove duplicates
sort data.txt | uniq
```

---

## üêõ Troubleshooting

### Container Build Issues
```bash
# Clear Docker cache
docker system prune -a

# Rebuild without cache
# F1 ‚Üí "Dev Containers: Rebuild Without Cache"
```

### Python Package Issues (Alpine)
```bash
# Some packages need build tools
sudo apk add gcc musl-dev python3-dev
```

### DuckDB Out of Memory
```bash
# Increase memory limit
duckdb -c "SET memory_limit='8GB'"
```

### Docker-in-Docker Not Working
```bash
# Check Docker is available
docker ps

# Restart if needed
sudo service docker restart
```

---

## üìö Documentation

- **README.md** - This file (overview, examples)
- **SETUP.md** - Detailed setup guide
- **.devcontainer/README.md** - Technical documentation
- **scripts/** - Inline code documentation

---

## üéØ Why This Environment?

### vs Cloud Data Platforms (Databricks, Snowflake)
- ‚úÖ **Free** - No cloud costs
- ‚úÖ **Portable** - Works anywhere (local, cloud, edge)
- ‚úÖ **Fast** - No cold starts, local processing
- ‚úÖ **Privacy** - Data stays local

### vs Heavy Containers (Jupyter, Anaconda)
- ‚úÖ **10-20x smaller** - 450MB vs 5-10GB
- ‚úÖ **Production-ready** - Not just notebooks
- ‚úÖ **Fast builds** - Minutes vs hours

### vs Single-Language
- ‚úÖ **Polyglot** - Python, Node.js, Bash, PowerShell
- ‚úÖ **Flexible** - Best tool for each task
- ‚úÖ **Future-proof** - Easy to extend

---

## üìù Environment Setup

```bash
# Copy template
cp .env.example .env

# Edit with your values
nano .env

# Common variables:
DB_HOST=localhost
DB_USER=tmj
DB_PASSWORD=secret
DELTA_LAKE_PATH=/workspace/data/delta
DUCKDB_MEMORY_LIMIT=4GB
```

---

## üí¨ Support

- üìñ Documentation in `.devcontainer/README.md`
- üêõ Issues on GitHub
- üí° Discussions for questions

---

## üèÜ Best For

‚úÖ Data engineers building ETL/ELT pipelines  
‚úÖ Analytics engineers using dbt  
‚úÖ Teams wanting reproducible workflows  
‚úÖ Edge/IoT data processing  
‚úÖ Cost-conscious projects  
‚úÖ Hybrid cloud/on-prem deployments  
‚úÖ Learning modern DataOps  

---

## üìù License

MIT License - Free for personal and commercial use

---

**Minimal Developer Environment - DataOps**  
*450MB. Production-ready. Scales to billions of rows.*

**User:** tmj (non-root, sudo enabled)  
**Made with ‚ù§Ô∏è for modern data engineering**
